{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectures 7 and 8: Building Autograd engine from scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning outcomes\n",
    "1. Understanding automated differentiatiom engines at a foundational level\n",
    "2. Operator overloading in OOP\n",
    "3. Graph representation\n",
    "4. Graph traversal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation, also known as forward pass or feedforward, is the process of moving input data through a neural network to generate an output. In other words, it's the process by which data is fed into the network, passes through its layers of neurons, and produces a prediction or classification.\n",
    "\n",
    "During forward propagation, the input data is multiplied by the weights of the first layer of neurons, and then passed through an activation function. The resulting output is then passed as input to the next layer, and the process is repeated until the final output is generated.\n",
    "\n",
    "The purpose of forward propagation is to produce a prediction or classification based on the input data. This prediction is compared to the actual output, and the error is used to adjust the weights of the network during backpropagation, which is the process of updating the weights in the network to improve its accuracy over time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation, also known as backward propagation or backprop, is the process of computing the gradient of the loss function with respect to the weights of a neural network. It is used to update the weights of the network during training, in order to minimize the error between the predicted output and the actual output.\n",
    "\n",
    "During backpropagation, the error between the predicted output and the actual output is propagated backwards through the network, layer by layer, in order to compute the gradient of the loss function with respect to the weights. This gradient is then used to update the weights of the network using an optimization algorithm such as stochastic gradient descent.\n",
    "\n",
    "The purpose of backpropagation is to adjust the weights of the network in such a way that the error between the predicted output and the actual output is minimized. This process is repeated iteratively during training, until the network converges to a set of weights that produce accurate predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A neural network\n",
    " is a type of machine learning algorithm that is modeled after the structure and function of the human brain. It consists of layers of interconnected nodes, or neurons, that process input data to generate output.\n",
    "\n",
    "The basic components of a neural network are:\n",
    "\n",
    "* Input layer: where the input data is fed into the network\n",
    "* Hidden layers: one or more layers of neurons that process the input data\n",
    "* Output layer: where the final output of the network is generated\n",
    "* Weights: numerical values that determine the strength of the connections between neurons\n",
    "* Activation function: a non-linear function applied to the output of each neuron to introduce non-linearity into the network\n",
    "\n",
    "...........\n",
    "\n",
    "\n",
    "Activation functions introduce non-linearity into a neural network by transforming the output of each neuron from a linear combination of inputs to a non-linear function of those inputs.\n",
    "\n",
    "To understand this concept, consider a simple linear regression model that takes an input x and produces an output y, given by the equation y = mx + b, where m is the slope of the line and b is the y-intercept. This is a linear function, meaning that the output y is a linear function of the input x.\n",
    "\n",
    "In contrast, the output of a neuron in a neural network is given by the equation y = f(wx + b), where f is the activation function, w is the weight vector, and b is the bias term. The activation function f is a non-linear function, which means that the output y is a non-linear function of the input x.\n",
    "\n",
    "This non-linearity is important because many real-world problems involve complex, non-linear relationships between inputs and outputs. For example, in image classification, the relationship between the pixels in an image and the class label of the image is highly non-linear.\n",
    "\n",
    "Without an activation function, a neural network would simply be a linear combination of the inputs, which would limit its ability to learn these non-linear relationships. By applying an activation function to the output of each neuron, the neural network can model these non-linear relationships and make more accurate predictions.\n",
    "\n",
    "Different activation functions have different shapes and properties, which affect how the network learns and how it behaves. For example, the sigmoid function has an S-shaped curve and is useful for binary classification problems, while the ReLU function is piecewise linear and is commonly used in deep neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Differentiation from Scratch Concept"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated differentiation is a key technique used in deep learning for efficiently computing gradients of complex functions. In this context, \"gradients\" refers to the partial derivatives of a function with respect to its inputs or parameters, which are used to update the parameters during the training process via gradient descent or similar optimization algorithms.\n",
    "\n",
    "There are two main approaches to implementing automated differentiation: forward mode and backward mode. Forward mode computes the derivatives of a function with respect to all of its inputs simultaneously, while backward mode computes the derivatives of the function with respect to a single input at a time. In practice, backward mode is generally more efficient and widely used in deep learning frameworks such as PyTorch and TensorFlow.\n",
    "\n",
    "To implement backward mode in Python from scratch, we can use a technique called \"reverse-mode differentiation\", also known as \"backpropagation\". Backpropagation works by breaking down a complex function into a series of simple, elementary operations, and then computing the derivative of the output with respect to each input in reverse order using the chain rule of calculus.\n",
    "\n",
    "Here is a high-level overview of the steps involved in implementing backpropagation from scratch:\n",
    "\n",
    "Define a computation graph for the function you want to differentiate, where each node in the graph represents an elementary operation and the edges represent the flow of inputs and outputs between nodes.\n",
    "During the forward pass, compute the output of the function and store intermediate values (e.g. input, output, and derivative values) at each node in the graph.\n",
    "During the backward pass, start at the output node and recursively compute the derivative of the output with respect to each input in the graph using the chain rule of calculus, while also propagating the derivative values backwards through the graph.\n",
    "Once the derivative values for each input have been computed, they can be used to update the parameters of the model via gradient descent or other optimization algorithms.\n",
    "This process can be somewhat complex and requires a deep understanding of calculus and graph theory. However, implementing backpropagation from scratch is a great exercise for learning the fundamental principles of deep learning and gaining a deeper understanding of how automatic differentiation works in practice."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
