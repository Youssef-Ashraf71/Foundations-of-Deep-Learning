{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lectures 7 & 8: Building Autograd engine from scratch "
      ],
      "metadata": {
        "id": "E-cCalx7Gkl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning outcomes\n",
        "1. Understanding Automated Differentiation Engines at a foundational level\n",
        "2. Operator Overloading in Object Oriented Programming\n",
        "3. Graph Representation\n",
        "4. Graph Traversal"
      ],
      "metadata": {
        "id": "tsrE-bJePIsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------"
      ],
      "metadata": {
        "id": "9o-ZtRBe8pR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automated Differentiation from Scratch as a Concept\n",
        "Automated differentiation is a key technique used in deep learning for efficiently computing gradients of complex functions. In this context, \"gradients\" refers to the partial derivatives of a function with respect to its inputs or parameters, which are used to update the parameters during the training process via gradient descent or similar optimization algorithms.\n",
        "\n",
        "There are two main approaches to implementing automated differentiation: forward mode and backward mode. Forward mode computes the derivatives of a function with respect to all of its inputs simultaneously, while backward mode computes the derivatives of the function with respect to a single input at a time. In practice, backward mode is generally more efficient and widely used in deep learning frameworks such as PyTorch and TensorFlow.\n",
        "\n",
        "To implement backward mode in Python from scratch, we can use a technique called \"reverse-mode differentiation\", also known as \"backpropagation\". Backpropagation works by breaking down a complex function into a series of simple, elementary operations, and then computing the derivative of the output with respect to each input in reverse order using the chain rule of calculus.\n"
      ],
      "metadata": {
        "id": "rcelFdWF6Fj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------"
      ],
      "metadata": {
        "id": "inHBg-ot8q7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "Restrictin yourself to <font color='green'>Python's Standard Library</font>, build an <font color='green'>autograd engine</font> capable of estimating the gradients required to solve the following problem using gradient descent.\n",
        "\n",
        ">> Find a point in the R<sup>2</sup> with the least average Euclidean distance to a set of arbitrary points. "
      ],
      "metadata": {
        "id": "utsSKljXqvCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the dataset and defining the loss function"
      ],
      "metadata": {
        "id": "eXXxjtIO53pc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tU4sNcntGfok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a33303-01ce-4822-c30f-a60586fd6b50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closed Form: gradient for x_p -0.5900849147094943, gradient for y_p -0.8073411877467226\n",
            "Closed Form: loss = 0.5472122517293468\n"
          ]
        }
      ],
      "source": [
        "from random import Random\n",
        "from math import sqrt\n",
        "SEED = 5\n",
        "def generate_pnts(N=1000):\n",
        "     random_gen = Random(x=SEED)\n",
        "     datax,datay=[],[]\n",
        "     for _ in range (N):\n",
        "          datax.append(random_gen.uniform(a=0,b=1))\n",
        "     for _ in range (N):\n",
        "          datay.append(random_gen.uniform(a=0,b=1))\n",
        "     return datax,datay\n",
        "\n",
        "def calc_grad(x_p,y_p,batch_x,batch_y):\n",
        "          sum_x=0;sum_y=0\n",
        "          # iterate over the elements of batch_x and batch_y in parallel\n",
        "          for x_i,y_i in zip(batch_x,batch_y):\n",
        "               # calculate the inverse square root of the distance between (x_i, y_i) and (x_p, y_p)\n",
        "              invsqrt = ((x_i-x_p)**2+(y_i-y_p)**2)**(-0.5)\n",
        "              # add the scaled difference between x_i and x_p to sum_x\n",
        "              sum_x+=invsqrt*(x_i-x_p);\n",
        "              # add the scaled difference between y_i and y_p to sum_y\n",
        "              sum_y+=invsqrt*(y_i-y_p);\n",
        "          return -sum_x/len(batch_x), -sum_y/len(batch_y)\n",
        "\n",
        "def loss(x_p , y_p,dx,dy):\n",
        "  # return the average of the square root of the sum of the squared differences\n",
        "  # between x_p and each element of dx and y_p and each element of dy\n",
        "     return (1/len(dx))*sum(\n",
        "         [sqrt((x_i - x_p)**2 + (y_i - y_p)**2 )\n",
        "         for x_i,y_i in zip(dx,dy)\n",
        "          ]\n",
        "     )\n",
        "\n",
        "data_x,data_y = generate_pnts(N=1)\n",
        "x_p,y_p = 0.3,0.3\n",
        "# call the calc_grad function with x_p, y_p, data_x, and data_y as arguments and store the results in grad_x and grad_y\n",
        "grad_x,grad_y = calc_grad(x_p,y_p,data_x,data_y)\n",
        "# call the loss function with x_p, y_p, data_x, and data_y as arguments and store the result in cur_loss\n",
        "cur_loss = loss(x_p,y_p,data_x,data_y)\n",
        "print(f\"Closed Form: gradient for x_p { grad_x}, gradient for y_p {grad_y}\")\n",
        "print(f\"Closed Form: loss = {cur_loss}\")\n",
        "\n",
        "\n",
        "\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch as an example of Autograd engines"
      ],
      "metadata": {
        "id": "XVgW_AVxhvSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "# Create a tensor with the initial point values and enable gradient tracking\n",
        "pnt = torch.tensor([0.3,0.3])\n",
        "pnt.requires_grad = True\n",
        "# Retain gradients for the tensor for future computations\n",
        "pnt.retain_grad()\n",
        "data_x,data_y = generate_pnts(N=1000)\n",
        "# Convert data to tensor and transpose it\n",
        "data = torch.tensor([data_x,data_y])\n",
        "# print(data.shape)  --> torch.Size([2, 1000])\n",
        "# Transposing is done for the industry convention\n",
        "# and to be able to broadcast pnt to [2,1000] and do subtraction\n",
        "data = data.t()  \n",
        "# print(data.shape,pnt.shape) --> torch.Size([1000, 2]) torch.Size([2])\n",
        "for i in range(1):\n",
        "      print(\"Iteration:\",i)\n",
        "      loss_torch =  torch.mean(torch.sqrt(((data-pnt)**2).sum(dim=1)))\n",
        "      print(f\"torch_Loss :{loss_torch}\")\n",
        "      # Compute gradients of the loss with respect to the tensor's values\n",
        "      loss_torch.backward()\n",
        "      print(f\"torch_grad:{pnt.grad.data}\")\n",
        "      pnt.grad.zero_()\n",
        "\n",
        "      \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFIefGr7h0bW",
        "outputId": "a2b6f7ff-98c5-4477-fd57-24d63e58ca36"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0\n",
            "torch_Loss :0.44305282831192017\n",
            "torch_grad:tensor([-0.3278, -0.3363])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code uses PyTorch library to compute the loss and its gradients for a given point pnt.\n",
        "\n",
        "\n",
        "* Import the PyTorch library\n",
        "* Create a PyTorch tensor named pnt with the initial point values (0.3, 0.3),\n",
        "* and enable gradient tracking for it\n",
        "* Retain the gradients for the tensor pnt\n",
        "* Generate 1000 random data points using the generate_pnts() function\n",
        "* Convert data points into a PyTorch tensor and transpose it for future computations\n",
        "* Calculate the loss using PyTorch's mean() and sqrt() functions\n",
        "* Print the loss calculated using PyTorch's functions\n",
        "* Compute gradients of the loss with respect to the tensor's values using the * backward() function\n",
        "* Print the gradient values for the tensor pnt"
      ],
      "metadata": {
        "id": "BOZ574TB5q0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Autograd from scratch\n"
      ],
      "metadata": {
        "id": "tOhTz8-g7GC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward propagation\n"
      ],
      "metadata": {
        "id": "JqLCtBoWwkJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward propagation, also known as forward pass or feedforward, is the process of moving input data through a neural network to generate an output. In other words, it's the process by which data is fed into the network, passes through its layers of neurons, and produces a prediction or classification.\n",
        "\n",
        "In the context of building an automated differentiation library, the forward pass involves computing the value of the function being differentiated and simultaneously building a computational graph that represents the function and the dependencies between its inputs and outputs.\n",
        "\n",
        "For each mathematical operation in the function (e.g. addition, multiplication, etc.), the forward pass creates a new node in the computational graph and connects it to the nodes corresponding to its inputs. The value of the output node is computed using the values of the input nodes and the mathematical operation.\n",
        "\n",
        "As the computational graph is being built during the forward pass, each node in the graph stores information about its value and the mathematical operation that created it, as well as references to the nodes representing its inputs. This information will be used during the backward pass to compute gradients.\n",
        "\n",
        "At the end of the forward pass, the output of the function being differentiated is returned along with the computational graph representing the function and its dependencies."
      ],
      "metadata": {
        "id": "0xw5Ud8GwlX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------"
      ],
      "metadata": {
        "id": "oBwGQiJZwog8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------"
      ],
      "metadata": {
        "id": "WouocFWswpPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backward propagation\n"
      ],
      "metadata": {
        "id": "oYUftZRgwrBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation, also known as backward propagation or backprop, is the process of computing the gradient of the loss function with respect to the weights of a neural network. It is used to update the weights of the network during training, in order to minimize the error between the predicted output and the actual output.\n",
        "\n",
        "During backpropagation, the error between the predicted output and the actual output is propagated backwards through the network, layer by layer, in order to compute the gradient of the loss function with respect to the weights. This gradient is then used to update the weights of the network using an optimization algorithm such as stochastic gradient descent.\n",
        "\n",
        "The purpose of backpropagation is to adjust the weights of the network in such a way that the error between the predicted output and the actual output is minimized. This process is repeated iteratively during training, until the network converges to a set of weights that produce accurate predictions."
      ],
      "metadata": {
        "id": "GGScl0Ikwt-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://user-images.githubusercontent.com/83988379/235384228-21253678-3126-4996-9701-767c8cb96c02.png)\n"
      ],
      "metadata": {
        "id": "25mexO3Ew64k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://user-images.githubusercontent.com/83988379/235384317-8183567f-178f-4080-8faa-44bb57f24878.png)\n"
      ],
      "metadata": {
        "id": "ohwppxJBxoUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how to calculate $\\frac{\\partial \\mathbb{L}}{\\partial x_p}$:\n",
        "<br><br>\n",
        "$$\n",
        "\\mathbb{L} = \\frac{1}{N} \\sum_{i=0}^{N-1}[(x_{i} - x_{p})^{2} + (y_{i} - y_{p})^{2}]^{\\frac{1}{2}}\n",
        "\\\\\n",
        "\\mathbb{L} = C \\sum_{i=0}^{N-1}{\\mathbb{L}(x_i, y_i)}\n",
        "\\\\\n",
        "where\\; C = \\frac{1}{N}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial x_p} = C \\sum_{i=0}^{N-1}{\\frac{\\partial \\mathbb{L}}{\\partial x_p}(x_i, y_i)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "Required\\; is\\; to\\; get: \\;\\;\\frac{\\partial \\mathbb{L}}{\\partial x_p},\\;\\; \\frac{\\partial \\mathbb{L}}{\\partial y_p}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbb{L} (x_i, y_i) = [(x_{i} - x_{p})^{2} + (y_{i} - y_{p})^{2}]^{\\frac{1}{2}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "Let \\;\\; I_x = x_i - x_p, \\;\\; I_y = y_i - y_p\n",
        "\\\\\n",
        "g_x = I_x^2, \\;\\; g_y = I_y^2\n",
        "\\\\\n",
        "M = g_x + g_y\n",
        "\\\\\n",
        "\\mathbb{L} = M^\\frac{1}{2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial I_x}{\\partial x_p} = -1,\n",
        "\\;\\; \\frac{\\partial I_y}{\\partial x_p} = 0\n",
        "\\\\\n",
        "\\frac{\\partial g_x}{\\partial I_x} = 2 I_x,\n",
        "\\;\\; \\frac{\\partial g_y}{\\partial I_y} = 2 I_y\n",
        "\\\\\n",
        "\\frac{\\partial M}{\\partial g_x} = 1,\n",
        "\\;\\; \\frac{\\partial M}{\\partial g_y} = 1\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial M} = \\frac{1}{2} M^\\frac{-1}{2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial g_x} = \\frac{\\partial \\mathbb{L}}{\\partial M}\\;\\;.\\;\\frac{\\partial M}{\\partial g_x}\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial g_x} = \\frac{1}{2} M^\\frac{-1}{2} \\;. \\; 1\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial I_x} = \\frac{\\partial \\mathbb{L}}{\\partial g_x}\\;\\;.\\;\\frac{\\partial g_x}{\\partial I_x}\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial g_x} = \\frac{1}{2} M^\\frac{-1}{2} \\;. \\; 2I_x\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial g_x} = M^\\frac{-1}{2} \\; . \\; I_x\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial x} = \\frac{\\partial \\mathbb{L}}{\\partial I_x}\\;\\;.\\;\\frac{\\partial I_x}{\\partial x_p}\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial x_p} = M^\\frac{-1}{2} \\; . \\; I_x \\; . \\; -1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial x_p} = -M^\\frac{-1}{2} \\; . \\; I_x\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial x_p} = -((x_i - x_p)^2 + (y_i - y_p)^2)^\\frac{-1}{2} \\;\\; . \\;\\; (x_i - x_p) = \\frac{\\partial \\mathbb{L}(x_i, y_i)}{\\partial g_x}\n",
        "$$"
      ],
      "metadata": {
        "id": "E2b02HCQ-cSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------"
      ],
      "metadata": {
        "id": "xJwmy-iwwxOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------"
      ],
      "metadata": {
        "id": "Ng74vApmwwm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement backward mode in Python from scratch, we can use a technique called \"reverse-mode differentiation\", also known as \"backpropagation\". Backpropagation works by breaking down a complex function into a series of simple, elementary operations, and then computing the derivative of the output with respect to each input in reverse order using the chain rule of calculus.\n",
        "\n"
      ],
      "metadata": {
        "id": "QifAlBUkvvQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a high-level overview of the steps involved in implementing backpropagation from scratch:\n",
        "\n",
        "Define a computation graph for the function you want to differentiate, where each node in the graph represents an elementary operation and the edges represent the flow of inputs and outputs between nodes.\n",
        "During the forward pass, compute the output of the function and store intermediate values (e.g. input, output, and derivative values) at each node in the graph.\n",
        "During the backward pass, start at the output node and recursively compute the derivative of the output with respect to each input in the graph using the chain rule of calculus, while also propagating the derivative values backwards through the graph.\n",
        "Once the derivative values for each input have been computed, they can be used to update the parameters of the model via gradient descent or other optimization algorithms.\n",
        "This process can be somewhat complex and requires a deep understanding of calculus and graph theory. However, implementing backpropagation from scratch is a great exercise for learning the fundamental principles of deep learning and gaining a deeper understanding of how automatic differentiation works in practice."
      ],
      "metadata": {
        "id": "vOQdxgsMvx6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the reccurence relation: Grad[i] = Grad[i-1]* Local Gradient\n",
        "\n",
        "or more accuaretly  Grad[node]  =  Grad[parent] * Local Gradient of the node \n",
        "\n",
        "<font color=\"green\">Local Gradient: \n",
        "$\\frac{\\partial(\\text{parent})}{\\partial(\\text{child})}$ </font>  \n",
        "\n",
        "                                      "
      ],
      "metadata": {
        "id": "chNBjFNPAWjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------"
      ],
      "metadata": {
        "id": "hNaxVkqniRYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------"
      ],
      "metadata": {
        "id": "Hr61rGvpiSIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Operator Overloading in Py"
      ],
      "metadata": {
        "id": "ioWTbA_jiU1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Operator overloading in Python allows developers to define how operators like +, -, *, /, ==, !=, <, >, and many others work with custom classes. By defining special methods with double underscores (e.g. __add__, __sub__, __mul__, __eq__, __ne__, __lt__, __gt__, etc.) in a class, developers can specify what should happen when the corresponding operator is used with instances of that class.\n",
        "\n",
        "For example, if you define a Vector class, you can define __add__ to add two vectors together, and then use the + operator to add instances of the Vector class. This makes it possible to write more expressive and natural-looking code that works with custom classes in the same way it works with built-in types like integers, floats, and strings.\n",
        "\n",
        "Operator overloading can be a powerful tool, but it should be used judiciously. Overusing operator overloading can lead to code that is hard to read and understand, so it's important to use it only when it makes the code more clear and expressive."
      ],
      "metadata": {
        "id": "FTLXkW45iSnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class comp_node:\n",
        "      def __init__(self, val , children = [] ,op = \"assign\"):\n",
        "          self.val = val\n",
        "          self.children = children\n",
        "          self.grad = 0 \n",
        "          self.op = op\n",
        "          self.backward_prop = lambda : None\n",
        "          self.typ = \"\"\n",
        "      def __to_comp_node__(self,obj):\n",
        "           if not isinstance(obj,comp_node):\n",
        "               return comp_node(val = obj)\n",
        "           else: \n",
        "               return obj      \n",
        "      def __sub__(self,other):\n",
        "           other = self.__to_comp_node__(other)\n",
        "           ret = comp_node( val = self.val - other.val ,\n",
        "                           children = [self,other],\n",
        "                           op =\"sub\")\n",
        "           def _backward_prop():\n",
        "                      self.grad  += ret.grad * 1\n",
        "                      other.grad += ret.grad * (-1)\n",
        "           ret.backward_prop = _backward_prop \n",
        "           return ret  \n",
        "\n",
        "      def __rsub__(self,other):\n",
        "           other = self.__to_comp_node__(other)\n",
        "           return other - self \n",
        "              \n",
        "           \n",
        "      def __add__(self,other):\n",
        "           other = self.__to_comp_node__(other)\n",
        "           ret = comp_node( val = self.val + other.val ,\n",
        "                           children = [self,other],\n",
        "                           op =\"add\")\n",
        "           def _backward_prop():\n",
        "                      self.grad  += ret.grad * 1\n",
        "                      other.grad += ret.grad * 1\n",
        "           ret.backward_prop = _backward_prop  \n",
        "           return ret  \n",
        "      def __radd__(self,other):\n",
        "           other = self.__to_comp_node__(other)\n",
        "           return other + self         \n",
        "      def __mul__(self,other):\n",
        "           other = self.__to_comp_node__(other)\n",
        "           ret = comp_node( val = self.val * other.val ,\n",
        "                           children = [self,other],\n",
        "                           op =\"mult\")\n",
        "           def _backward_prop():\n",
        "                self.grad += ret.grad * other.val  ## y=x*z >> y'wrt x = d(x)*z = 1*z ,, y'wrt z = d(z)*x = 1*x ||  x ->self , z ->other\n",
        "                other.grad += ret.grad * self.val  ## y'wrt z = d(z)*x = 1*x   \n",
        "           return ret\n",
        "\n",
        "      def __rmul__(self,other):\n",
        "           other = self.__to_comp_node__(other)\n",
        "           return other * self            \n",
        "      def __pow__(self , exponent):\n",
        "          if not isinstance(exponent , (int , float)):\n",
        "              raise ValueError(\"Unsuppoerted Datatypes\")\n",
        "          ret = comp_node(val = self.val**exponent ,\n",
        "                          children = [self],\n",
        "                          op = f\"power  {exponent}\")         \n",
        "          def _backward_prop():\n",
        "                      self.grad  += ret.grad * (exponent * self.val**(exponent - 1))   # L = M ^(1/2)  --->  out > L -- self - > child -> M  --  out -> parent -> L |||  child += parent * Local  \n",
        "          ret.backward_prop = _backward_prop  \n",
        "          return ret\n",
        "\n",
        "      def __eq__(self,other):\n",
        "             return self.val == other.val  \n",
        "      def __repr__(self):\n",
        "           return (f\"op: {self.op} || Val: {self.val:.4f} || # children: {len(self.children)} || grad = {self.grad} \")         \n",
        "\n",
        "      def __truediv__(self, other):\n",
        "              other = self.__to_comp_node__(other)\n",
        "              if other.val == 0:\n",
        "                raise ValueError(\"can not divide by zero (undefined)\")\n",
        "              out = comp_node(val=self.val / other.val, children=[self, other], op=\"div\")\n",
        "              def _backward_prop():\n",
        "                # d(u/v)/dx = (v*du/dx - u*dv/dx) / v^2 ---> For the first operand 'self'    \n",
        "                  self.grad += out.grad / other.val   # dl/dself = 1*(1/other) \n",
        "                # d(u/v)/dv = -u/(v^2) ----> For the second operand 'other'\n",
        "                  other.grad -= out.grad * self.val / (other.val**2)  # # dl/dself = 1*(-1)(other)**-2 = 1*(-1) / ((other)**2) \n",
        "              out.backward_prop = _backward_prop\n",
        "              return out\n",
        "      def __rtruediv__(self, other):\n",
        "              other = self.__to_comp_node__(other)\n",
        "              return other / self   \n",
        "          \n",
        "      def sin(self):\n",
        "              out = comp_node(val=np.sin(self.val), children=[self], op=\"sin\")\n",
        "              def _backward_prop():\n",
        "                  self.grad += out.grad * np.cos(self.val)\n",
        "              out.backward_prop = _backward_prop\n",
        "              return out\n",
        "          \n",
        "      def cos(self):\n",
        "              out = comp_node(val=np.cos(self.val), children=[self], op=\"cos\")\n",
        "              def _backward_prop():\n",
        "                  self.grad -= out.grad * np.sin(self.val)\n",
        "              out.backward_prop = _backward_prop\n",
        "              return out\n",
        "assert comp_node(val = 5  ).val == 5 , \" Assignment Failed :/\" \n",
        "assert (comp_node( val = 5) - comp_node(val = 3)).val == 2\n",
        "assert (comp_node( val = 5) - 3).val == 2\n",
        "assert (3 - comp_node( val = 5)).val == -2\n",
        "assert (comp_node(val = 5)**2).val == 25\n",
        "assert (comp_node( val = 5 )**2) == comp_node( val = 25)\n",
        "assert (comp_node(val = 5)+comp_node(val = 3)).val == 8\n",
        "assert (comp_node(val = 5) + 3).val == 8  \n",
        "assert (3 + comp_node(val = 5)).val == 8  \n",
        "assert (comp_node(val = 5) * 3).val == 15  \n",
        "assert (3 * comp_node(val = 5)).val == 15\n",
        "assert ( comp_node(val = 15)/3).val == 5\n",
        "assert ( 15 / comp_node(val = 3)).val == 5\n",
        "####################################\n",
        "\n",
        "\n",
        "data_x,data_y = generate_pnts(N=1)\n",
        "x_p,y_p = comp_node(val=0.3),comp_node(val=0.3) \n",
        "def loss_graph(x_p,y_p,data_x,data_y):\n",
        "          I_x,I_y = x_p-data_x, y_p-data_y\n",
        "          g_x,g_y = I_x**2,I_y**2\n",
        "          M = g_x+g_y\n",
        "          l = M**(0.5)\n",
        "          return l,[l,M,g_x,g_y,I_x,I_y,x_p,y_p]\n",
        "\n",
        "\n",
        "ans,rev_top_order = loss_graph(x_p,y_p,data_x[0],data_y[0])\n",
        "# print(ans,\"\\n\")  -> op: power: (0.5) || Val: 0.5472 || children: 1 \n",
        "rev_top_order[0].grad = 1\n",
        "print(\"Traversing the Graph nodes in reverse top sort order\\n\")\n",
        "for i, node in enumerate(rev_top_order):\n",
        "      node.backward_prop()\n",
        "      print(i , node)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAVyruGJ2TqL",
        "outputId": "9eee5b18-d8e9-4acd-9b62-fa08aba17a45"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traversing the Graph nodes in reverse top sort order\n",
            "\n",
            "0 op: power  0.5 || Val: 0.5472 || # children: 1 || grad = 1 \n",
            "1 op: add || Val: 0.2994 || # children: 2 || grad = 0.9137222319490423 \n",
            "2 op: power  2 || Val: 0.1043 || # children: 1 || grad = 0.9137222319490423 \n",
            "3 op: power  2 || Val: 0.1952 || # children: 1 || grad = 0.9137222319490423 \n",
            "4 op: sub || Val: -0.3229 || # children: 2 || grad = -0.5900849147094943 \n",
            "5 op: sub || Val: -0.4418 || # children: 2 || grad = -0.8073411877467226 \n",
            "6 op: assign || Val: 0.3000 || # children: 0 || grad = -0.5900849147094943 \n",
            "7 op: assign || Val: 0.3000 || # children: 0 || grad = -0.8073411877467226 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The \"__sub__\" method is used when the subtraction operator is applied to the instance of the class, while \"__rsub__\" is used when the subtraction operator is applied to the other operand first, and the instance of the class is the second operand.\n",
        "\n",
        "In the case of __rsub__, the backward propagation function is not necessary because the gradient of the first operand (the other operand) with respect to the output of the subtraction operation is already computed by its own __sub__ method. Therefore, we don't need to compute the gradients again in __rsub__.\n",
        "\n",
        "In summary, we implemented the backward propagation function for __sub__ but not for __rsub__ because the gradients needed for __rsub__ are already computed by the __sub__ method of the other operand."
      ],
      "metadata": {
        "id": "hHphFanyldgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------"
      ],
      "metadata": {
        "id": "iHD3q64clehp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines a class comp_node that represents a computational node in a computation graph.\n",
        " The class has the following attributes:\n",
        "* val: the value of the node (Assigning)\n",
        "* children: a list of child nodes ( similar to adjacecny list )\n",
        "* grad: the gradient of the node (initialized to 0)\n",
        "* op: the operation performed to compute the node's value (e.g., \"add\", \"subtract\", etc.)\n",
        "\n",
        "The class has several special methods that overload the behavior of operators such as +, -, *, and **. These methods create new comp_node instances that represent the result of applying the corresponding operation to the operands.\n",
        "The __to_comp_node__ method is a helper method that converts a given object to a comp_node instance if necessary.\n",
        "The code includes several assertions to test the correctness of the implementation.\n",
        "\n",
        "Overall, the code provides a basic implementation of automatic differentiation using the forward mode. It allows the user to define a computation graph and automatically compute the gradients of the nodes with respect to a given output node."
      ],
      "metadata": {
        "id": "Axc0rIcIwPqI"
      }
    }
  ]
}